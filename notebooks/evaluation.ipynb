{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jan-HeinKok/machinelearning2/blob/main/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7j15nHUG_iZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)  # 4 labels: cheap, affordable, expensive, neutral\n",
        "\n",
        "# Convert DataFrame to Dataset object\n",
        "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621,
          "referenced_widgets": [
            "1402f683baa44745985c1e851e2e2270",
            "83df0a7c7fb24803a5a25a3ba390f8e6",
            "8d736166863e4203b53cb97e6f2d2146",
            "c6e5703bb3a743c189a192b69de3d0fb",
            "d235cdd3dee54fecb114c6b4cf2ca99c",
            "dd5b450837ac4a46a87b1c826593be0e",
            "85ce890ea9f94476919f35d644ce3c88",
            "cdcb525e63a645c1bf9cf7b94ada042e",
            "f40ec97e6fed4dce8249971698bb5164",
            "8e11136384b649b1bd6a080e2a3b233e",
            "98fd8d49cf904d5ea9227e823b8582e6",
            "d962ecdef7fb4a6b908b31a71dc4da91",
            "f978bed58dfa4a49a7b60f30094bfaf6",
            "0d2416bcd4844de3a89c48d612b94b1a",
            "a0a581668bfe4cf7922aae13bb185af8",
            "5ba6d40a84e94d15bf4fbf0eb9fe7dbe",
            "10be6a9af032438a8a1e233379a3407e",
            "1649e436e89f472d97668524d642f7be",
            "c86c7a8908104984b3acfcb15aa9cc39",
            "16ac99d582f345fd99213f14b5dd592a",
            "0bdbdd95d28d4ab79e3e855eb7f3cb60",
            "07c3447ca0574b058d9d876bd0e715eb",
            "bf69ef758e27498ea4d5e24f65527a52",
            "4744b7a06b73453cb6c3a57b6e557bec",
            "a14c4982943c4da5b0d691d3d4861a54",
            "6e46d21a8831465d9577958a97733c91",
            "7ca22dc33924403ea8846f2e0ea32ec0",
            "1540f333e8b145f58f796c678860f434",
            "81ba163c8b94438c8b2c2e2d24379860",
            "4a819fc97a354c878f80665d63dfc3fb",
            "07b1950264af4899a2a7264015bb7e9a",
            "904694092fb14567b7f46fa178efc55c",
            "f053120191164d9cacaa83541669c226",
            "5941e3b302d84748a171c57848f7d9f2",
            "e795af2d52cb45c3afc1284c4da8f6ea",
            "f5bd1157008e4f2a865a27b72bdff04c",
            "ae3c13d068604630aeb3cda9e7c69061",
            "af2ba46aea6a4143a3d1370b61275f2d",
            "2e4ae0673769481e8a0dc102ed077275",
            "71d501565f5f45bc99bc6430885d3bf1",
            "f8240d5fe80e4c179f690750091f5083",
            "068b8d915eea405497a3a26188d40c0a",
            "510013422492434fad9d9f91b448cf69",
            "84a6371106d8410daa677e02bc9d19eb",
            "ff85331c15e14976a3dc28d889ea76a4",
            "4d5d167d547148d4b7f4d98f6e545e62",
            "8666dc7cdd874700a34a7611adae2bd0",
            "e5a4297f21c54aa3aaa0fd05e33b9599",
            "82f7ce098788456daaffd97267149234",
            "5e831f326c294b9788503e63c389f6b1",
            "228438051660426391d79e97d5911385",
            "bce3e3b0a3874b64b8fe9fec1973ccdb",
            "c83463a818a64aafa3352a08432e1627",
            "a12bf5dbed50410b9ae676ecf0692902",
            "e9ddc1f25b2c4605814d8fa5b4f56870",
            "8082cbb8220145c1b652b33f6303167f",
            "f69a39d95c844b06ade2173a1575203e",
            "8c3c82ad0bb7412e993878bf98494fbd",
            "de16f05c85e54d33aa60398c735df4e4",
            "244b0251f69c4b9a9d5bbc288633c679",
            "64bb9de6223c48e19b357ba2c9071c31",
            "5b0d4296e2d549b29d2979bdaeda9244",
            "e9e4b28dd1564b0f8dc2ac18aadb36e0",
            "935a040f0cea4d24b4290385860c4007",
            "a5cd49c03dca4677862361ccd49ec9ac",
            "641320c323654164a5411344634f75f3"
          ]
        },
        "id": "_Cgn4cnn_2o1",
        "outputId": "585cd39b-5b41-41cf-f831-5eae4213ac71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1402f683baa44745985c1e851e2e2270"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d962ecdef7fb4a6b908b31a71dc4da91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf69ef758e27498ea4d5e24f65527a52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5941e3b302d84748a171c57848f7d9f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/713 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff85331c15e14976a3dc28d889ea76a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/179 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8082cbb8220145c1b652b33f6303167f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [270/270 03:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.271400</td>\n",
              "      <td>1.177275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.073100</td>\n",
              "      <td>1.009536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.867100</td>\n",
              "      <td>0.833209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=270, training_loss=1.1023459823043258, metrics={'train_runtime': 206.7397, 'train_samples_per_second': 10.346, 'train_steps_per_second': 1.306, 'total_flos': 562804653625344.0, 'train_loss': 1.1023459823043258, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ZYHnvB-Pip5U",
        "outputId": "cc5bceff-915c-4fa0-bad5-4ab07bb3500b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23/23 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.8332086205482483, 'eval_runtime': 5.4075, 'eval_samples_per_second': 33.102, 'eval_steps_per_second': 4.253, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsKa6IWxLBsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/quikr_car.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_data(df):\n",
        "    df = df.dropna(subset=['Price'])\n",
        "    df = df.rename(columns={'name': 'name', 'company': 'company', 'year': 'year', 'kms_driven': 'kms_driven', 'fuel_type': 'fuel_type', 'Price': 'Price'})\n",
        "    df = df[['name', 'company', 'year', 'kms_driven', 'fuel_type', 'Price']]\n",
        "    df = df.dropna()\n",
        "    df = df[df['Price'] != 'Ask For Price']  # Remove rows with 'Ask For Price'\n",
        "    df['Price'] = df['Price'].str.replace(',', '').astype(float)\n",
        "\n",
        "    bins = [0, 300000, 600000, float('inf')]\n",
        "    labels = ['cheap', 'affordable', 'expensive']\n",
        "    df['label'] = pd.cut(df['Price'], bins=bins, labels=labels, right=False)\n",
        "    df = df.dropna(subset=['label'])\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess_data(df)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Split data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['name'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': train_labels\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    'input_ids': val_encodings['input_ids'],\n",
        "    'attention_mask': val_encodings['attention_mask'],\n",
        "    'labels': val_labels\n",
        "})\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,  # Increase the number of epochs\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    learning_rate=2e-5,  # Experiment with different learning rates\n",
        "    evaluation_strategy='epoch'\n",
        ")\n",
        "\n",
        "# Define data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define compute metrics function\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"BERT Model Evaluation Results: {eval_results}\")\n",
        "\n",
        "# Define and train a baseline model for comparison\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Prepare data for baseline model\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_val = vectorizer.transform(val_texts)\n",
        "\n",
        "# Train baseline model\n",
        "baseline_model = MultinomialNB()\n",
        "baseline_model.fit(X_train, train_labels)\n",
        "\n",
        "# Evaluate baseline model\n",
        "baseline_preds = baseline_model.predict(X_val)\n",
        "baseline_accuracy = accuracy_score(val_labels, baseline_preds)\n",
        "baseline_precision, baseline_recall, baseline_f1, _ = precision_recall_fscore_support(val_labels, baseline_preds, average='weighted')\n",
        "\n",
        "print(f\"Baseline Model Accuracy: {baseline_accuracy}\")\n",
        "print(f\"Baseline Model Precision: {baseline_precision}\")\n",
        "print(f\"Baseline Model Recall: {baseline_recall}\")\n",
        "print(f\"Baseline Model F1 Score: {baseline_f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mJ-as07-QSbv",
        "outputId": "6553130f-8dd9-47e8-9129-7a3ad054c6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [410/410 00:36, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.068308</td>\n",
              "      <td>0.481707</td>\n",
              "      <td>0.370593</td>\n",
              "      <td>0.481707</td>\n",
              "      <td>0.418905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.947947</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.597487</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.425761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.829162</td>\n",
              "      <td>0.628049</td>\n",
              "      <td>0.713229</td>\n",
              "      <td>0.628049</td>\n",
              "      <td>0.616006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.761790</td>\n",
              "      <td>0.664634</td>\n",
              "      <td>0.684319</td>\n",
              "      <td>0.664634</td>\n",
              "      <td>0.671555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.720994</td>\n",
              "      <td>0.707317</td>\n",
              "      <td>0.730911</td>\n",
              "      <td>0.707317</td>\n",
              "      <td>0.711352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Model Evaluation Results: {'eval_loss': 0.7209937572479248, 'eval_accuracy': 0.7073170731707317, 'eval_precision': 0.7309110641457703, 'eval_recall': 0.7073170731707317, 'eval_f1': 0.7113515415402207, 'eval_runtime': 0.2883, 'eval_samples_per_second': 568.833, 'eval_steps_per_second': 72.838, 'epoch': 5.0}\n",
            "Baseline Model Accuracy: 0.6829268292682927\n",
            "Baseline Model Precision: 0.687831110249377\n",
            "Baseline Model Recall: 0.6829268292682927\n",
            "Baseline Model F1 Score: 0.6787266640361045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyGmIB7YL0As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets scikit-learn torch\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load and inspect the dataset\n",
        "df = pd.read_csv('/content/quikr_car.csv')\n",
        "\n",
        "# Print the column names and the first few rows of the DataFrame\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_data(df):\n",
        "    # Adjust these column names if they are different in your CSV file\n",
        "    column_mapping = {\n",
        "        'name': 'name',          # Adjust if the column name is different\n",
        "        'company': 'company',    # Adjust if the column name is different\n",
        "        'year': 'year',          # Adjust if the column name is different\n",
        "        'kms_driven': 'kms_driven',  # Adjust if the column name is different\n",
        "        'fuel_type': 'fuel_type',    # Adjust if the column name is different\n",
        "        'Price': 'Price'            # Adjust if the column name is different\n",
        "    }\n",
        "    df = df.rename(columns=column_mapping)\n",
        "\n",
        "    df = df[['name', 'company', 'year', 'kms_driven', 'fuel_type', 'Price']]\n",
        "    df = df.dropna()\n",
        "    df['Price'] = df['Price'].apply(lambda x: 'neutral' if x == 'Ask For Price' else x)\n",
        "    df['Price'] = df['Price'].str.replace('â‚¹', '').str.replace(',', '')\n",
        "\n",
        "    # Convert 'Price' to numeric, invalid parsing will be set as NaN\n",
        "    df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
        "\n",
        "    # Drop rows where 'Price' is NaN\n",
        "    df = df.dropna(subset=['Price'])\n",
        "\n",
        "    bins = [0, 300000, 600000, float('inf')]\n",
        "    labels = ['cheap', 'affordable', 'expensive']\n",
        "    df['label'] = pd.cut(df['Price'], bins=bins, labels=labels, right=False)\n",
        "    df = df.dropna(subset=['label'])\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "df = preprocess_data(df)\n",
        "\n",
        "# Encode labels\n",
        "label_mapping = {'cheap': 0, 'affordable': 1, 'expensive': 2, 'neutral': 3}\n",
        "df['label'] = df['label'].map(label_mapping)\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['name'], df['label'], test_size=0.2)\n",
        "\n",
        "# Fine-tune BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)\n",
        "\n",
        "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': train_labels})\n",
        "val_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask'], 'labels': val_labels})\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model('./fine_tuned_bert')\n",
        "\n",
        "# Evaluate the fine-tuned BERT model\n",
        "results = trainer.evaluate()\n",
        "print(\"BERT Model Evaluation Results:\", results)\n",
        "\n",
        "# Baseline Model using Logistic Regression\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_val = vectorizer.transform(val_texts)\n",
        "\n",
        "baseline_model = LogisticRegression(max_iter=1000)\n",
        "baseline_model.fit(X_train, train_labels)\n",
        "\n",
        "baseline_preds = baseline_model.predict(X_val)\n",
        "baseline_accuracy = accuracy_score(val_labels, baseline_preds)\n",
        "baseline_precision, baseline_recall, baseline_f1, _ = precision_recall_fscore_support(val_labels, baseline_preds, average='weighted')\n",
        "\n",
        "print(f\"Baseline Model Accuracy: {baseline_accuracy}\")\n",
        "print(f\"Baseline Model Precision: {baseline_precision}\")\n",
        "print(f\"Baseline Model Recall: {baseline_recall}\")\n",
        "print(f\"Baseline Model F1 Score: {baseline_f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mkYoWzcVWdSI",
        "outputId": "2d26d20e-18f4-498f-9188-c0dd2e2fd472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Index(['name', 'company', 'year', 'Price', 'kms_driven', 'fuel_type'], dtype='object')\n",
            "                                     name   company  year          Price  \\\n",
            "0    Hyundai Santro Xing XO eRLX Euro III   Hyundai  2007         80,000   \n",
            "1                 Mahindra Jeep CL550 MDI  Mahindra  2006       4,25,000   \n",
            "2              Maruti Suzuki Alto 800 Vxi    Maruti  2018  Ask For Price   \n",
            "3  Hyundai Grand i10 Magna 1.2 Kappa VTVT   Hyundai  2014       3,25,000   \n",
            "4        Ford EcoSport Titanium 1.5L TDCi      Ford  2014       5,75,000   \n",
            "\n",
            "   kms_driven fuel_type  \n",
            "0  45,000 kms    Petrol  \n",
            "1      40 kms    Diesel  \n",
            "2  22,000 kms    Petrol  \n",
            "3  28,000 kms    Petrol  \n",
            "4  36,000 kms    Diesel  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [246/246 00:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.047800</td>\n",
              "      <td>1.051968</td>\n",
              "      <td>0.493902</td>\n",
              "      <td>0.440165</td>\n",
              "      <td>0.493902</td>\n",
              "      <td>0.369756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.952000</td>\n",
              "      <td>0.808390</td>\n",
              "      <td>0.695122</td>\n",
              "      <td>0.703148</td>\n",
              "      <td>0.695122</td>\n",
              "      <td>0.651291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.630300</td>\n",
              "      <td>0.815598</td>\n",
              "      <td>0.701220</td>\n",
              "      <td>0.705927</td>\n",
              "      <td>0.701220</td>\n",
              "      <td>0.679311</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Model Evaluation Results: {'eval_loss': 0.8155977129936218, 'eval_accuracy': 0.7012195121951219, 'eval_precision': 0.7059267229397277, 'eval_recall': 0.7012195121951219, 'eval_f1': 0.6793107442933226, 'eval_runtime': 0.2964, 'eval_samples_per_second': 553.393, 'eval_steps_per_second': 37.118, 'epoch': 3.0}\n",
            "Baseline Model Accuracy: 0.774390243902439\n",
            "Baseline Model Precision: 0.7734409899044045\n",
            "Baseline Model Recall: 0.774390243902439\n",
            "Baseline Model F1 Score: 0.7678332788267895\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM+kXdBd3U5plGBDVLKgKQb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
